{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e1799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile Name: US Meta Llama 3.2 11B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-2-11b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.2 90B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-2-90b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.2 3B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-2-3b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.2 1B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-2-1b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.1 8B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.1 70B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Meta Llama 3.3 70B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama3-3-70b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Llama 4 Maverick 17B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama4-maverick-17b-instruct-v1:0\n",
      "---\n",
      "Profile Name: US Llama 4 Scout 17B Instruct\n",
      "Profile ARN: arn:aws:bedrock:us-west-2:055029294644:inference-profile/us.meta.llama4-scout-17b-instruct-v1:0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "\n",
    "response = bedrock_client.list_inference_profiles()\n",
    "for profile in response['inferenceProfileSummaries']:\n",
    "    if \"llama\" in profile['inferenceProfileName'].lower():\n",
    "        print(f\"Profile Name: {profile['inferenceProfileName']}\")\n",
    "        print(f\"Profile ARN: {profile['inferenceProfileArn']}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3bd2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '99734254-31ad-4871-b044-3eeae12072a4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 14 Jul 2025 17:09:27 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '4131',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '99734254-31ad-4871-b044-3eeae12072a4'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-8b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-1-8b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.1 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'DISTILLATION'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-70b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-1-70b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.1 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'DISTILLATION'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-11b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-2-11b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.2 11B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-90b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-2-90b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.2 90B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-1b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-2-1b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.2 1B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'DISTILLATION'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-3b-instruct-v1:0:128k',\n",
       "   'modelId': 'meta.llama3-2-3b-instruct-v1:0:128k',\n",
       "   'modelName': 'Llama 3.2 3B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'DISTILLATION'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "bedrock_client.list_foundation_models(byProvider=\"meta\", byCustomizationType=\"FINE_TUNING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2b3696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 21:35:09 [INFO] Progress: 10 / 1000\n",
      "2025-07-14 21:35:27 [INFO] Progress: 20 / 1000\n",
      "2025-07-14 21:35:43 [INFO] Progress: 30 / 1000\n",
      "2025-07-14 21:36:07 [WARNING] Expected 10 lines, got 90 → retry batch\n",
      "2025-07-14 21:36:39 [INFO] Progress: 40 / 1000\n",
      "2025-07-14 21:37:09 [INFO] Progress: 50 / 1000\n",
      "2025-07-14 21:37:28 [WARNING] Expected 10 lines, got 11 → retry batch\n",
      "2025-07-14 21:37:34 [WARNING] Throttled (ThrottlingException). Retrying in 0.41s …\n",
      "2025-07-14 21:37:41 [WARNING] Throttled (ThrottlingException). Retrying in 0.97s …\n",
      "2025-07-14 21:38:23 [INFO] Progress: 60 / 1000\n",
      "2025-07-14 21:38:44 [INFO] Progress: 70 / 1000\n",
      "2025-07-14 21:39:20 [INFO] Progress: 80 / 1000\n",
      "2025-07-14 21:39:40 [WARNING] Expected 10 lines, got 11 → retry batch\n",
      "2025-07-14 21:40:07 [WARNING] Expected 10 lines, got 90 → retry batch\n",
      "2025-07-14 21:40:50 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 21:40:50 [INFO] Progress: 89 / 1000\n",
      "2025-07-14 21:41:19 [INFO] Progress: 99 / 1000\n",
      "2025-07-14 21:41:57 [WARNING] Expected 10 lines, got 134 → retry batch\n",
      "2025-07-14 21:42:18 [WARNING] Expected 10 lines, got 86 → retry batch\n",
      "2025-07-14 21:42:49 [WARNING] Expected 10 lines, got 122 → retry batch\n",
      "2025-07-14 21:43:35 [WARNING] Expected 10 lines, got 181 → retry batch\n",
      "2025-07-14 21:43:55 [INFO] Progress: 109 / 1000\n",
      "2025-07-14 21:44:17 [INFO] Progress: 119 / 1000\n",
      "2025-07-14 21:44:42 [INFO] Progress: 129 / 1000\n",
      "2025-07-14 21:45:29 [INFO] Progress: 139 / 1000\n",
      "2025-07-14 21:46:01 [INFO] Progress: 149 / 1000\n",
      "2025-07-14 21:46:41 [INFO] Progress: 159 / 1000\n",
      "2025-07-14 21:47:02 [INFO] Progress: 169 / 1000\n",
      "2025-07-14 21:47:35 [INFO] Progress: 179 / 1000\n",
      "2025-07-14 21:47:55 [INFO] Progress: 189 / 1000\n",
      "2025-07-14 21:48:39 [INFO] Progress: 199 / 1000\n",
      "2025-07-14 21:49:20 [INFO] Progress: 209 / 1000\n",
      "2025-07-14 21:49:52 [INFO] Progress: 219 / 1000\n",
      "2025-07-14 21:50:13 [INFO] Progress: 229 / 1000\n",
      "2025-07-14 21:50:48 [INFO] Progress: 239 / 1000\n",
      "2025-07-14 21:51:39 [WARNING] Expected 10 lines, got 194 → retry batch\n",
      "2025-07-14 21:52:08 [WARNING] Expected 10 lines, got 109 → retry batch\n",
      "2025-07-14 21:52:31 [INFO] Progress: 249 / 1000\n",
      "2025-07-14 21:52:51 [INFO] Progress: 259 / 1000\n",
      "2025-07-14 21:53:22 [INFO] Progress: 269 / 1000\n",
      "2025-07-14 21:53:38 [INFO] Progress: 279 / 1000\n",
      "2025-07-14 21:53:58 [INFO] Progress: 289 / 1000\n",
      "2025-07-14 21:54:08 [WARNING] Throttled (ThrottlingException). Retrying in 0.42s …\n",
      "2025-07-14 21:54:34 [INFO] Progress: 299 / 1000\n",
      "2025-07-14 21:54:44 [WARNING] Throttled (ThrottlingException). Retrying in 0.57s …\n",
      "2025-07-14 21:55:09 [INFO] Progress: 309 / 1000\n",
      "2025-07-14 21:55:36 [WARNING] Expected 10 lines, got 30 → retry batch\n",
      "2025-07-14 21:56:05 [INFO] Progress: 319 / 1000\n",
      "2025-07-14 21:56:15 [WARNING] Throttled (ThrottlingException). Retrying in 0.50s …\n",
      "2025-07-14 21:56:54 [INFO] Progress: 329 / 1000\n",
      "2025-07-14 21:57:31 [WARNING] Expected 10 lines, got 158 → retry batch\n",
      "2025-07-14 21:58:11 [INFO] Progress: 339 / 1000\n",
      "2025-07-14 21:58:44 [WARNING] Expected 10 lines, got 126 → retry batch\n",
      "2025-07-14 21:59:23 [INFO] Progress: 349 / 1000\n",
      "2025-07-14 22:00:07 [INFO] Progress: 359 / 1000\n",
      "2025-07-14 22:00:29 [INFO] Progress: 369 / 1000\n",
      "2025-07-14 22:01:18 [WARNING] Expected 10 lines, got 189 → retry batch\n",
      "2025-07-14 22:01:45 [INFO] Progress: 379 / 1000\n",
      "2025-07-14 22:02:02 [INFO] Progress: 389 / 1000\n",
      "2025-07-14 22:02:38 [INFO] Progress: 399 / 1000\n",
      "2025-07-14 22:03:08 [INFO] Progress: 409 / 1000\n",
      "2025-07-14 22:03:24 [INFO] Progress: 419 / 1000\n",
      "2025-07-14 22:03:36 [INFO] Progress: 429 / 1000\n",
      "2025-07-14 22:03:47 [WARNING] Throttled (ThrottlingException). Retrying in 0.56s …\n",
      "2025-07-14 22:04:29 [INFO] Progress: 439 / 1000\n",
      "2025-07-14 22:04:52 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 22:04:52 [INFO] Progress: 448 / 1000\n",
      "2025-07-14 22:05:23 [INFO] Progress: 458 / 1000\n",
      "2025-07-14 22:05:45 [WARNING] Expected 10 lines, got 85 → retry batch\n",
      "2025-07-14 22:06:30 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 22:06:30 [INFO] Progress: 467 / 1000\n",
      "2025-07-14 22:07:03 [INFO] Progress: 477 / 1000\n",
      "2025-07-14 22:07:20 [INFO] Progress: 487 / 1000\n",
      "2025-07-14 22:07:52 [INFO] Progress: 497 / 1000\n",
      "2025-07-14 22:08:21 [INFO] Progress: 507 / 1000\n",
      "2025-07-14 22:09:00 [WARNING] Expected 10 lines, got 11 → retry batch\n",
      "2025-07-14 22:09:27 [INFO] Progress: 517 / 1000\n",
      "2025-07-14 22:10:03 [INFO] Progress: 527 / 1000\n",
      "2025-07-14 22:10:18 [INFO] Progress: 537 / 1000\n",
      "2025-07-14 22:10:47 [WARNING] Expected 10 lines, got 11 → retry batch\n",
      "2025-07-14 22:11:02 [INFO] Progress: 547 / 1000\n",
      "2025-07-14 22:11:11 [WARNING] Throttled (ThrottlingException). Retrying in 0.43s …\n",
      "2025-07-14 22:11:16 [WARNING] Throttled (ThrottlingException). Retrying in 0.91s …\n",
      "2025-07-14 22:12:05 [INFO] Progress: 557 / 1000\n",
      "2025-07-14 22:12:41 [WARNING] Expected 10 lines, got 30 → retry batch\n",
      "2025-07-14 22:12:52 [INFO] Progress: 567 / 1000\n",
      "2025-07-14 22:13:31 [INFO] Progress: 577 / 1000\n",
      "2025-07-14 22:13:45 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 22:13:46 [INFO] Progress: 586 / 1000\n",
      "2025-07-14 22:14:07 [INFO] Progress: 596 / 1000\n",
      "2025-07-14 22:14:16 [WARNING] Throttled (ThrottlingException). Retrying in 0.43s …\n",
      "2025-07-14 22:14:59 [WARNING] Expected 10 lines, got 127 → retry batch\n",
      "2025-07-14 22:15:16 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 22:15:16 [INFO] Progress: 605 / 1000\n",
      "2025-07-14 22:15:36 [INFO] Progress: 615 / 1000\n",
      "2025-07-14 22:16:13 [INFO] Progress: 625 / 1000\n",
      "2025-07-14 22:16:52 [INFO] Progress: 635 / 1000\n",
      "2025-07-14 22:17:26 [WARNING] Expected 10 lines, got 121 → retry batch\n",
      "2025-07-14 22:17:52 [WARNING] Expected 10 lines, got 97 → retry batch\n",
      "2025-07-14 22:18:11 [INFO] Progress: 645 / 1000\n",
      "2025-07-14 22:18:53 [WARNING] Expected 10 lines, got 128 → retry batch\n",
      "2025-07-14 22:19:32 [INFO] Progress: 655 / 1000\n",
      "2025-07-14 22:20:00 [INFO] Progress: 665 / 1000\n",
      "2025-07-14 22:20:49 [INFO] Progress: 675 / 1000\n",
      "2025-07-14 22:21:10 [WARNING] Expected 10 lines, got 79 → retry batch\n",
      "2025-07-14 22:21:35 [INFO] Progress: 685 / 1000\n",
      "2025-07-14 22:22:04 [WARNING] Malformed JSON skipped.\n",
      "2025-07-14 22:22:04 [INFO] Progress: 694 / 1000\n",
      "2025-07-14 22:22:49 [INFO] Progress: 704 / 1000\n",
      "2025-07-14 22:23:02 [WARNING] Expected 10 lines, got 52 → retry batch\n",
      "2025-07-14 22:23:25 [INFO] Progress: 714 / 1000\n",
      "2025-07-14 22:23:47 [WARNING] Expected 10 lines, got 86 → retry batch\n",
      "2025-07-14 22:24:41 [INFO] Progress: 724 / 1000\n",
      "2025-07-14 22:24:55 [INFO] Progress: 734 / 1000\n",
      "2025-07-14 22:25:23 [INFO] Progress: 744 / 1000\n",
      "2025-07-14 22:25:43 [WARNING] Expected 10 lines, got 79 → retry batch\n",
      "2025-07-14 22:25:49 [WARNING] Throttled (ThrottlingException). Retrying in 0.57s …\n",
      "2025-07-14 22:26:28 [INFO] Progress: 754 / 1000\n",
      "2025-07-14 22:26:46 [WARNING] Expected 10 lines, got 30 → retry batch\n",
      "2025-07-14 22:27:15 [INFO] Progress: 764 / 1000\n",
      "2025-07-14 22:27:22 [WARNING] Throttled (ThrottlingException). Retrying in 0.49s …\n",
      "2025-07-14 22:27:43 [INFO] Progress: 774 / 1000\n",
      "2025-07-14 22:27:51 [WARNING] Throttled (ThrottlingException). Retrying in 0.54s …\n",
      "2025-07-14 22:28:39 [INFO] Progress: 784 / 1000\n",
      "2025-07-14 22:29:05 [INFO] Progress: 794 / 1000\n",
      "2025-07-14 22:29:40 [INFO] Progress: 804 / 1000\n",
      "2025-07-14 22:30:11 [INFO] Progress: 814 / 1000\n",
      "2025-07-14 22:30:34 [INFO] Progress: 824 / 1000\n",
      "2025-07-14 22:30:53 [INFO] Progress: 834 / 1000\n",
      "2025-07-14 22:31:17 [INFO] Progress: 844 / 1000\n",
      "2025-07-14 22:31:42 [INFO] Progress: 854 / 1000\n",
      "2025-07-14 22:31:48 [WARNING] Throttled (ThrottlingException). Retrying in 0.50s …\n",
      "2025-07-14 22:31:53 [WARNING] Throttled (ThrottlingException). Retrying in 1.11s …\n",
      "2025-07-14 22:32:16 [INFO] Progress: 864 / 1000\n",
      "2025-07-14 22:32:42 [INFO] Progress: 874 / 1000\n",
      "2025-07-14 22:32:46 [WARNING] Throttled (ThrottlingException). Retrying in 0.54s …\n",
      "2025-07-14 22:32:54 [WARNING] Throttled (ThrottlingException). Retrying in 1.07s …\n",
      "2025-07-14 22:33:35 [WARNING] Expected 10 lines, got 132 → retry batch\n",
      "2025-07-14 22:34:00 [INFO] Progress: 884 / 1000\n",
      "2025-07-14 22:34:35 [INFO] Progress: 894 / 1000\n",
      "2025-07-14 22:35:05 [WARNING] Expected 10 lines, got 110 → retry batch\n",
      "2025-07-14 22:35:33 [INFO] Progress: 904 / 1000\n",
      "2025-07-14 22:35:52 [WARNING] Expected 10 lines, got 71 → retry batch\n",
      "2025-07-14 22:36:12 [INFO] Progress: 914 / 1000\n",
      "2025-07-14 22:36:18 [WARNING] Throttled (ThrottlingException). Retrying in 0.50s …\n",
      "2025-07-14 22:36:25 [WARNING] Throttled (ThrottlingException). Retrying in 1.01s …\n",
      "2025-07-14 22:37:09 [INFO] Progress: 924 / 1000\n",
      "2025-07-14 22:37:29 [WARNING] Expected 10 lines, got 84 → retry batch\n",
      "2025-07-14 22:37:48 [INFO] Progress: 934 / 1000\n",
      "2025-07-14 22:38:07 [INFO] Progress: 944 / 1000\n",
      "2025-07-14 22:38:18 [WARNING] Throttled (ThrottlingException). Retrying in 0.54s …\n",
      "2025-07-14 22:38:27 [WARNING] Throttled (ThrottlingException). Retrying in 1.08s …\n",
      "2025-07-14 22:38:55 [WARNING] Expected 10 lines, got 110 → retry batch\n",
      "2025-07-14 22:39:41 [INFO] Progress: 954 / 1000\n",
      "2025-07-14 22:40:18 [INFO] Progress: 964 / 1000\n",
      "2025-07-14 22:40:45 [WARNING] Expected 10 lines, got 95 → retry batch\n",
      "2025-07-14 22:41:10 [INFO] Progress: 974 / 1000\n",
      "2025-07-14 22:41:47 [INFO] Progress: 984 / 1000\n",
      "2025-07-14 22:42:02 [INFO] Progress: 994 / 1000\n",
      "2025-07-14 22:42:17 [INFO] Progress: 1000 / 1000\n",
      "2025-07-14 22:42:17 [INFO] Dataset saved → D:\\Study\\UNIVERSITY\\Competition\\VPB\\vpbank-hackathon\\financial_ai_finetuning\\jar_coaching_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"ap-southeast-2\")\n",
    "os.environ[\"AWS_PROFILE\"] = \"hackathon\"\n",
    "MODEL_ID = os.getenv(\n",
    "    \"BEDROCK_MODEL_ID\",\n",
    "    \"arn:aws:bedrock:ap-southeast-2:055029294644:inference-profile/apac.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    ")\n",
    "TOTAL_RECORDS    = 1000\n",
    "BATCH_SIZE       = 10         # number of samples per Bedrock call\n",
    "OUTPUT_FILE      = Path(\"jar_coaching_dataset.jsonl\")\n",
    "MAX_RETRIES      = 3\n",
    "BACKOFF_BASE_SEC = 0.5\n",
    "\n",
    "SPENDING_JARS    = [\"NEC\", \"PLY\"]\n",
    "ENCOURAGE_JARS   = [\"FFA\", \"EDU\", \"GIV\"]\n",
    "ALL_JARS         = SPENDING_JARS + ENCOURAGE_JARS\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "\n",
    "\n",
    "def bedrock_with_retry(client, **kwargs) -> dict:\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            return client.converse(**kwargs)\n",
    "        except ClientError as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                logger.error(\"Max retries reached → %s\", e)\n",
    "                raise\n",
    "            backoff = BACKOFF_BASE_SEC * (2 ** (attempt - 1)) * random.uniform(0.8, 1.2)\n",
    "            logger.warning(\"Throttled (%s). Retrying in %.2fs …\", e.response[\"Error\"][\"Code\"], backoff)\n",
    "            time.sleep(backoff)\n",
    "\n",
    "\n",
    "# PROMPT TEMPLATE (kept identical to inference-time prompt)\n",
    "PROMPT_TEMPLATE = \"\"\"Human: Bạn là một trợ lý tài chính AI vui tính, nhí nhảnh hài hước nhưng vẫn rất thông minh. Nhiệm vụ của bạn là phân tích dữ liệu chi tiêu từ các \"lọ\" (jars) của người dùng và đưa ra lời khuyên hoặc cảnh báo sắc bén dưới dạng một danh sách JSON.\n",
    "\n",
    "**QUY TẮC:**\n",
    "1.  **Phân tích:**\n",
    "    - Với các lọ chi tiêu thông thường (NEC, PLY), nếu `actual_daily_spending` < `ideal_daily_spending`, đây là vấn đề cần cảnh báo (chi tiêu quá nhanh).\n",
    "    - Với các lọ khuyến khích (FFA, EDU, GIV), nếu `actual_daily_spending` > `ideal_daily_spending`, đây là vấn đề cần nhắc nhở (chi tiêu quá chậm).\n",
    "2.  **Định dạng đầu ra:** Phải là một danh sách (array) các đối tượng JSON hợp lệ, không chứa bất kỳ văn bản nào khác ngoài danh sách JSON này.\n",
    "3.  **Cấu trúc đối tượng JSON:** Mỗi đối tượng phải chứa các key: `jar_code`, `issue`, `recommendation`, `priority`.\n",
    "    - `jar_code`: Mã của lọ (ví dụ: \"PLY\").\n",
    "    - `issue`: Mô tả ngắn gọn vấn đề (\"Chi tiêu cao bất thường\" hoặc \"Chi tiêu thấp bất thường\").\n",
    "    - `recommendation`: Một lời khuyên/cảnh báo ngắn gọn, hài hước, thông minh bằng tiếng Việt.\n",
    "    - `priority`: Mức độ ưu tiên (\"high\", \"medium\", hoặc \"low\").\n",
    "\n",
    "**DỮ LIỆU CẦN PHÂN TÍCH:**\n",
    "<data>\n",
    "{jar_summary}\n",
    "</data>\n",
    "\n",
    "Bây giờ, hãy tạo danh sách JSON dựa trên dữ liệu trên.\n",
    "\n",
    "Assistant:\n",
    "```json\n",
    "\"\"\"\n",
    "\n",
    "# data synthesis functions\n",
    "def vnd(val: int) -> str:\n",
    "    return f\"{val:,.0f}đ\".replace(\",\", \".\")\n",
    "\n",
    "\n",
    "def random_jar_line(code: str) -> tuple[str, int, int]:\n",
    "    \"\"\"Return a jar summary line that **triggers** the rule.\"\"\"\n",
    "    while True:\n",
    "        ideal = random.randint(100_000, 1_000_000)\n",
    "        actual = int(ideal * random.uniform(0.5, 1.5))\n",
    "\n",
    "        # enforce trigger condition with 120 % threshold\n",
    "        if code in SPENDING_JARS and actual < ideal * 1.2:\n",
    "            break\n",
    "        if code in ENCOURAGE_JARS and actual > ideal * 1.2:\n",
    "            break\n",
    "\n",
    "    line = (\n",
    "        f\"- Lọ {code}: Mức chi tiêu lý tưởng mỗi ngày là {vnd(ideal)}, \"\n",
    "        f\"nhưng mức chi tiêu thực tế còn lại mỗi ngày của bạn là {vnd(actual)}.\"\n",
    "    )\n",
    "    return line, ideal, actual\n",
    "\n",
    "\n",
    "def build_prompt() -> tuple[str, dict]:\n",
    "    \"\"\"Generate ONE scenario prompt and structured jar data.\"\"\"\n",
    "    n_jars = random.randint(1, len(ALL_JARS))\n",
    "    picked = random.sample(ALL_JARS, k=n_jars)\n",
    "\n",
    "    lines, jars_data = [], []\n",
    "    for code in picked:\n",
    "        l, ideal, actual = random_jar_line(code)\n",
    "        lines.append(l)\n",
    "        jars_data.append(\n",
    "            {\"jar_code\": code, \"ideal_daily_spending\": ideal, \"actual_daily_spending\": actual}\n",
    "        )\n",
    "\n",
    "    prompt_str = PROMPT_TEMPLATE.format(jar_summary=\"\\n\".join(lines))\n",
    "    return prompt_str, jars_data\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # OUTPUT_FILE.unlink(missing_ok=True)\n",
    "    done = 0\n",
    "\n",
    "    while done < TOTAL_RECORDS:\n",
    "        batch_size = min(BATCH_SIZE, TOTAL_RECORDS - done)\n",
    "        scenario_prompts: List[str] = []\n",
    "        for _ in range(batch_size):\n",
    "            p, _ = build_prompt()\n",
    "            scenario_prompts.append(p)\n",
    "\n",
    "        # Build user prompt asking the model to answer each scenario on its own line\n",
    "        BATCH_USER_PROMPT = \"\\n\\n\".join(\n",
    "            f\"### SCENARIO {idx+1}\\n{scenario_prompts[idx]}\" for idx in range(batch_size)\n",
    "        )\n",
    "        clarification = (\n",
    "            \"Trả về đúng \"\n",
    "            f\"{batch_size} kết quả, mỗi kết quả trên **một dòng duy nhất**, \"\n",
    "            \"theo đúng thứ tự các SCENARIO, và không kèm bất kỳ ký tự nào khác.\"\n",
    "        )\n",
    "\n",
    "        user_prompt = BATCH_USER_PROMPT + \"\\n\\n\" + clarification\n",
    "\n",
    "        # bedrock call\n",
    "        resp = bedrock_with_retry(\n",
    "            bedrock_rt,\n",
    "            modelId=MODEL_ID,\n",
    "            system=[{\"text\": \"You are a digital assistant with a friendly personality\"}],\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}],\n",
    "        )\n",
    "        raw = resp[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "\n",
    "        # align answers with prompts\n",
    "        answers = [l for l in raw.splitlines() if l.strip()]\n",
    "        if len(answers) != batch_size:\n",
    "            logger.warning(\"Expected %d lines, got %d → retry batch\", batch_size, len(answers))\n",
    "            continue\n",
    "\n",
    "        # validate and write to file\n",
    "        with OUTPUT_FILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            for p, c in zip(scenario_prompts, answers):\n",
    "                # clean accidental code-fences\n",
    "                if c.startswith(\"```\"):\n",
    "                    c = c.strip(\"`\").lstrip(\"json\").strip()\n",
    "                try:\n",
    "                    json.loads(c)\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(\"Malformed JSON skipped.\")\n",
    "                    continue\n",
    "                f.write(json.dumps({\"prompt\": p, \"completion\": c}, ensure_ascii=False) + \"\\n\")\n",
    "                done += 1\n",
    "\n",
    "        logger.info(\"Progress: %d / %d\", done, TOTAL_RECORDS)\n",
    "\n",
    "    logger.info(\"Dataset saved → %s\", OUTPUT_FILE.resolve())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc652ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 00:35:50 [INFO] Loading cached SSO token for test_sess\n",
      "2025-07-15 00:36:17 [INFO] Progress: 10 / 600\n",
      "2025-07-15 00:36:52 [INFO] Progress: 20 / 600\n",
      "2025-07-15 00:37:31 [INFO] Progress: 30 / 600\n",
      "2025-07-15 00:38:09 [INFO] Progress: 40 / 600\n",
      "2025-07-15 00:38:25 [INFO] Progress: 50 / 600\n",
      "2025-07-15 00:39:00 [INFO] Progress: 60 / 600\n",
      "2025-07-15 00:39:27 [INFO] Progress: 70 / 600\n",
      "2025-07-15 00:40:03 [INFO] Progress: 80 / 600\n",
      "2025-07-15 00:40:43 [INFO] Progress: 90 / 600\n",
      "2025-07-15 00:41:15 [INFO] Progress: 100 / 600\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"ap-southeast-2\")\n",
    "os.environ[\"AWS_PROFILE\"] = \"hackathon\"\n",
    "MODEL_ID = os.getenv(\n",
    "    \"BEDROCK_MODEL_ID\",\n",
    "    \"arn:aws:bedrock:ap-southeast-2:055029294644:inference-profile/apac.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    ")\n",
    "TOTAL_RECORDS    = 600\n",
    "BATCH_SIZE       = 10         # number of samples per Bedrock call\n",
    "OUTPUT_FILE      = Path(\"jar_coaching_dataset.jsonl\")\n",
    "MAX_RETRIES      = 3\n",
    "BACKOFF_BASE_SEC = 0.5\n",
    "\n",
    "SPENDING_JARS    = [\"NEC\", \"PLY\"]\n",
    "ENCOURAGE_JARS   = [\"FFA\", \"EDU\", \"GIV\"]\n",
    "ALL_JARS         = SPENDING_JARS + ENCOURAGE_JARS\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "\n",
    "\n",
    "def bedrock_with_retry(client, **kwargs) -> dict:\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            return client.converse(**kwargs)\n",
    "        except ClientError as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                logger.error(\"Max retries reached → %s\", e)\n",
    "                raise\n",
    "            backoff = BACKOFF_BASE_SEC * (2 ** (attempt - 1)) * random.uniform(0.8, 1.2)\n",
    "            logger.warning(\"Throttled (%s). Retrying in %.2fs …\", e.response[\"Error\"][\"Code\"], backoff)\n",
    "            time.sleep(backoff)\n",
    "\n",
    "\n",
    "# PROMPT TEMPLATE (kept identical to inference-time prompt)\n",
    "PROMPT_TEMPLATE = \"\"\"Human: Bạn là một trợ lý tài chính AI vui tính, nhí nhảnh hài hước nhưng vẫn rất thông minh. Nhiệm vụ của bạn là phân tích dữ liệu chi tiêu từ các \"lọ\" (jars) của người dùng và đưa ra lời khuyên hoặc cảnh báo sắc bén dưới dạng một danh sách JSON.\n",
    "\n",
    "**QUY TẮC:**\n",
    "1.  **Phân tích:**\n",
    "    - Với các lọ chi tiêu thông thường (NEC, PLY), nếu `actual_daily_spending` < `ideal_daily_spending`, đây là vấn đề cần cảnh báo (chi tiêu quá nhanh).\n",
    "    - Với các lọ khuyến khích (FFA, EDU, GIV), nếu `actual_daily_spending` > `ideal_daily_spending`, đây là vấn đề cần nhắc nhở (chi tiêu quá chậm).\n",
    "2.  **Định dạng đầu ra:** Phải là một danh sách (array) các đối tượng JSON hợp lệ, không chứa bất kỳ văn bản nào khác ngoài danh sách JSON này.\n",
    "3.  **Cấu trúc đối tượng JSON:** Mỗi đối tượng phải chứa các key: `jar_code`, `issue`, `recommendation`, `priority`.\n",
    "    - `jar_code`: Mã của lọ (ví dụ: \"PLY\").\n",
    "    - `issue`: Mô tả ngắn gọn vấn đề (\"Chi tiêu cao bất thường\" hoặc \"Chi tiêu thấp bất thường\").\n",
    "    - `recommendation`: Một lời khuyên/cảnh báo ngắn gọn, hài hước, thông minh bằng tiếng Việt.\n",
    "    - `priority`: Mức độ ưu tiên (\"high\", \"medium\", hoặc \"low\").\n",
    "\n",
    "**DỮ LIỆU CẦN PHÂN TÍCH:**\n",
    "<data>\n",
    "{jar_summary}\n",
    "</data>\n",
    "\n",
    "Bây giờ, hãy tạo danh sách JSON dựa trên dữ liệu trên.\n",
    "\n",
    "Assistant:\n",
    "```json\n",
    "\"\"\"\n",
    "\n",
    "# data synthesis functions\n",
    "def vnd(val: int) -> str:\n",
    "    return f\"{val:,.0f}đ\".replace(\",\", \".\")\n",
    "\n",
    "\n",
    "def random_jar_line(code: str) -> tuple[str, int, int]:\n",
    "    \"\"\"Return a jar summary line that **triggers** the rule.\"\"\"\n",
    "    while True:\n",
    "        ideal = random.randint(100_000, 1_000_000)\n",
    "        actual = int(ideal * random.uniform(0.5, 1.5))\n",
    "\n",
    "        # enforce trigger condition with 120 % threshold\n",
    "        if code in SPENDING_JARS and actual < ideal * 1.2:\n",
    "            break\n",
    "        if code in ENCOURAGE_JARS and actual > ideal * 1.2:\n",
    "            break\n",
    "\n",
    "    line = (\n",
    "        f\"- Lọ {code}: Mức chi tiêu lý tưởng mỗi ngày là {vnd(ideal)}, \"\n",
    "        f\"nhưng mức chi tiêu thực tế còn lại mỗi ngày của bạn là {vnd(actual)}.\"\n",
    "    )\n",
    "    return line, ideal, actual\n",
    "\n",
    "\n",
    "def build_prompt() -> tuple[str, dict]:\n",
    "    \"\"\"Generate ONE scenario prompt and structured jar data.\"\"\"\n",
    "    n_jars = random.randint(1, len(ALL_JARS))\n",
    "    picked = random.sample(ALL_JARS, k=n_jars)\n",
    "\n",
    "    lines, jars_data = [], []\n",
    "    for code in picked:\n",
    "        l, ideal, actual = random_jar_line(code)\n",
    "        lines.append(l)\n",
    "        jars_data.append(\n",
    "            {\"jar_code\": code, \"ideal_daily_spending\": ideal, \"actual_daily_spending\": actual}\n",
    "        )\n",
    "\n",
    "    prompt_str = PROMPT_TEMPLATE.format(jar_summary=\"\\n\".join(lines))\n",
    "    return prompt_str, jars_data\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # OUTPUT_FILE.unlink(missing_ok=True)\n",
    "    done = 0\n",
    "\n",
    "    while done < TOTAL_RECORDS:\n",
    "        batch_size = min(BATCH_SIZE, TOTAL_RECORDS - done)\n",
    "        scenario_prompts: List[str] = []\n",
    "        for _ in range(batch_size):\n",
    "            p, _ = build_prompt()\n",
    "            scenario_prompts.append(p)\n",
    "\n",
    "        # Build user prompt asking the model to answer each scenario on its own line\n",
    "        BATCH_USER_PROMPT = \"\\n\\n\".join(\n",
    "            f\"### SCENARIO {idx+1}\\n{scenario_prompts[idx]}\" for idx in range(batch_size)\n",
    "        )\n",
    "        clarification = (\n",
    "            \"Trả về đúng \"\n",
    "            f\"{batch_size} kết quả, mỗi kết quả trên **một dòng duy nhất**, \"\n",
    "            \"theo đúng thứ tự các SCENARIO, và không kèm bất kỳ ký tự nào khác.\"\n",
    "        )\n",
    "\n",
    "        user_prompt = BATCH_USER_PROMPT + \"\\n\\n\" + clarification\n",
    "\n",
    "        # bedrock call\n",
    "        resp = bedrock_with_retry(\n",
    "            bedrock_rt,\n",
    "            modelId=MODEL_ID,\n",
    "            system=[{\"text\": \"You are a digital assistant with a friendly personality\"}],\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}],\n",
    "        )\n",
    "        raw = resp[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "\n",
    "        # align answers with prompts\n",
    "        answers = [l for l in raw.splitlines() if l.strip()]\n",
    "        if len(answers) != batch_size:\n",
    "            logger.warning(\"Expected %d lines, got %d → retry batch\", batch_size, len(answers))\n",
    "            continue\n",
    "\n",
    "        # validate and write to file\n",
    "        with OUTPUT_FILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            for p, c in zip(scenario_prompts, answers):\n",
    "                # clean accidental code-fences\n",
    "                if c.startswith(\"```\"):\n",
    "                    c = c.strip(\"`\").lstrip(\"json\").strip()\n",
    "                try:\n",
    "                    json.loads(c)\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(\"Malformed JSON skipped.\")\n",
    "                    continue\n",
    "                f.write(json.dumps({\"prompt\": p, \"completion\": c}, ensure_ascii=False) + \"\\n\")\n",
    "                done += 1\n",
    "\n",
    "        logger.info(\"Progress: %d / %d\", done, TOTAL_RECORDS)\n",
    "\n",
    "    logger.info(\"Dataset saved → %s\", OUTPUT_FILE.resolve())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4976df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
